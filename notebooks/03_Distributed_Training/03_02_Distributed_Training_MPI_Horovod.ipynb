{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of traditional distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard distributed TensorFlow package runs with a parameter server approach to averaging gradients. In this approach, each process has one of two potential roles: a worker or a parameter server. Workers process the training data, compute gradients, and send them to parameter servers to be averaged.\n",
    "\n",
    "Challenges of Tensorflow distributed training\n",
    "\n",
    "- __Identifying the right ratio of worker to parameter servers__. If one parameter server is used, it will likely become a networking or computational bottleneck. If multiple parameter servers are used, the communication pattern becomes “all-to-all” which may saturate network interconnects.\n",
    "\n",
    "- __Bandwidth is not optimal, network could be the bottleneck__. If model has more parameters, network traffic grow with number of parameters. This is not a scalable solution for large neural network. \n",
    "\n",
    "- __Handling increased TensorFlow program complexity__. User has to explicitly start each worker and parameter server, pass around service discovery information such as hosts and ports of all the workers and parameter servers, and modify the training program to construct `tf.Server()` with an appropriate `tf.ClusterSpec()`. Additionally, users had to ensure that all the operations were placed appropriately using `tf.train.device_replica_setter()` and code is modified to use towers to leverage multiple GPUs within the server. This often led to a steep learning curve and a significant amount of code restructuring, taking time away from the actual modeling.\n",
    "\n",
    "> Note: Complexity problem has been address by TF-operator, end user doesn't need to worry about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Ring Allreduce\n",
    "In early 2017, Baidu published an article, “Bringing HPC Techniques to Deep Learning,” evangelizing a different algorithm for averaging gradients and communicating those gradients to all nodes. \n",
    "\n",
    "In the `ring-allreduce` algorithm, each of N nodes communicates with two of its peers 2*(N-1) times. During this communication, a node sends and receives chunks of the data buffer. In the first N-1 iterations, received values are added to the values in the node’s buffer. In the second N-1 iterations, received values replace the values held in the node’s buffer. Baidu’s paper suggests that this algorithm is bandwidth-optimal, meaning that if the buffer is large enough, it will optimally utilize the available network.\n",
    "\n",
    "![allreduce](./images/allreducering.png)\n",
    "\n",
    "\n",
    "# MPI\n",
    "Users utilize a Message Passing Interface (MPI) implementation such as Open MPI to launch all copies of the TensorFlow program. MPI then transparently sets up the distributed infrastructure necessary for workers to communicate with each other. All the user needs to do is modify their program to average gradients using an `allreduce()` operation.\n",
    "\n",
    "\n",
    "# Horovod\n",
    "\n",
    "The realization that a `ring-allreduce` approach can improve both __usability__ and __performance__ motivated us to work on our own implementation to address Uber’s TensorFlow needs. Uber adopted Baidu’s draft implementation of the TensorFlow ring-allreduce algorithm and built upon it. That's horovod.\n",
    "\n",
    "Hovorod now supports most of the popular deep learning frameworks like Tensorflow, PyTorch and MxNet.\n",
    "\n",
    "\n",
    "![allreduce-scale](./images/allreduce-scale.png)\n",
    "The number of samples processed per second with a 300-million parameter language model scales linearly with the number of GPUs concurrently doing synchronous training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "We will use MPI + Horovod + Tensorflow to train image classification model in this notebook.\n",
    "\n",
    "This job requires GPU nodes in the EKS cluster, you have two options to scale up the GPU nodes\n",
    "\n",
    "1. Open EC2 portal and find your GPU ASG, change desired number.\n",
    "2. Use eksctl command `eksctl scale nodegroup --cluster=<your_eks_cluster_name> --nodes=3 <GPU_node_group_name>` \n",
    "3. Install node autoscaler and it will scale up and down the EKS cluster based on resource requested. (separate tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat distributed-training-jobs/distributed-mpi-job.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Resources\n",
    "\n",
    "`resource.limits.nvidia.com/gpu: 4` means every container will use 4 GPUs. \n",
    "> Note: Make sure you use p2.8xlarge or p3.8xlarge with 4 gpus at least\n",
    "\n",
    "`replicas: 2` means we totally want to use 2 containers. \n",
    "\n",
    "In total, there're 8 gpus form a ring and doing distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create -f distributed-training-jobs/distributed-mpi-job.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get mpijob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe mpijob distributed-mpi-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pod | grep distributed-mpi-job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check logs of Launcher Job\n",
    "\n",
    "Every mpi job will create a launcher job and it will sync with all workers and persist logs. \n",
    "\n",
    "You will see some logs like this\n",
    "\n",
    "\n",
    "```\n",
    "90\timages/sec: 813.1 +/- 4.6 (jitter = 29.8)\t7.593\n",
    "90\timages/sec: 813.1 +/- 4.6 (jitter = 29.4)\t7.562\n",
    "....\n",
    "90\timages/sec: 813.1 +/- 4.9 (jitter = 25.7)\t7.572\n",
    "100\timages/sec: 814.2 +/- 4.5 (jitter = 25.0)\t7.579\n",
    "----------------------------------------------------------------\n",
    "total images/sec: 6511.22\n",
    "----------------------------------------------------------------\n",
    "100\timages/sec: 814.1 +/- 4.3 (jitter = 29.0)\t7.604\n",
    "----------------------------------------------------------------\n",
    "total images/sec: 6511.26\n",
    "----------------------------------------------------------------\n",
    "100\timages/sec: 814.1 +/- 4.3 (jitter = 29.2)\t7.549\n",
    "----------------------------------------------------------------\n",
    "total images/sec: 6511.20\n",
    "----------------------------------------------------------------\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl logs distributed-mpi-job-launcher-q6h2d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "GPU instance are every expensive, remember to scale down the nodes when you finish the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}