{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chicago Taxi Example using Tensorflow Extended (not ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chicago Taxi example demonstrates the end-to-end workflow and steps of how to analyze, validate and transform data, train a model, analyze and serve it. It uses:\n",
    "\n",
    "\n",
    "Note: This site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at oneâ€™s own risk.\n",
    "\n",
    "\n",
    "## The dataset\n",
    "\n",
    "This sample is based on the model-analysis example \n",
    "[here](https://github.com/tensorflow/tfx/tree/master/tfx/examples/chicago_taxi).\n",
    "\n",
    "The sample trains and analyzes a model based on the \n",
    "[Taxi Trips dataset](https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew)\n",
    "released by the City of Chicago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubeflow website has a very detail expaination of kubeflow components, please go to [Introduction to the Pipelines SDK](https://www.kubeflow.org/docs/pipelines/sdk/sdk-overview/) for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX doesn't support S3 now and all data have to be shared in a network file system. We need to prepare a persistvolume for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import datetime\n",
    "from kubernetes.client.models import V1EnvVar\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "def dataflow_tf_data_validation_op(inference_data: 'GcsUri', validation_data: 'GcsUri', column_names: 'GcsUri[text/json]', key_columns, project: 'GcpProject', mode, validation_output: 'GcsUri[Directory]', step_name='validation'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tfdv:a277f87ea1d4707bf860d080d06639b7caf9a1cf',\n",
    "        arguments = [\n",
    "            '--csv-data-for-inference', inference_data,\n",
    "            '--csv-data-to-validate', validation_data,\n",
    "            '--column-names', column_names,\n",
    "            '--key-columns', key_columns,\n",
    "            '--project', project,\n",
    "            '--mode', mode,\n",
    "            '--output', '%s/{{workflow.name}}/validation' % validation_output,\n",
    "        ],\n",
    "        file_outputs = {\n",
    "            'schema': '/schema.txt',\n",
    "            'validation': '/output_validation_result.txt',\n",
    "        }\n",
    "    )\n",
    "\n",
    "def dataflow_tf_transform_op(train_data: 'GcsUri', evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', project: 'GcpProject', preprocess_mode, preprocess_module: 'GcsUri[text/code/python]', transform_output: 'GcsUri[Directory]', step_name='preprocess'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tft:a277f87ea1d4707bf860d080d06639b7caf9a1cf',\n",
    "        arguments = [\n",
    "            '--train', train_data,\n",
    "            '--eval', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--project', project,\n",
    "            '--mode', preprocess_mode,\n",
    "            '--preprocessing-module', preprocess_module,\n",
    "            '--output', '%s/{{workflow.name}}/transformed' % transform_output,\n",
    "        ],\n",
    "        file_outputs = {'transformed': '/output.txt'}\n",
    "    )\n",
    "\n",
    "\n",
    "def tf_train_op(transformed_data_dir, schema: 'GcsUri[text/json]', learning_rate: float, hidden_layer_size: int, steps: int, target: str, preprocess_module: 'GcsUri[text/code/python]', training_output: 'GcsUri[Directory]', step_name='training'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-kubeflow-tf-trainer:a277f87ea1d4707bf860d080d06639b7caf9a1cf',\n",
    "        arguments = [\n",
    "            '--transformed-data-dir', transformed_data_dir,\n",
    "            '--schema', schema,\n",
    "            '--learning-rate', learning_rate,\n",
    "            '--hidden-layer-size', hidden_layer_size,\n",
    "            '--steps', steps,\n",
    "            '--target', target,\n",
    "            '--preprocessing-module', preprocess_module,\n",
    "            '--job-dir', '%s/{{workflow.name}}/train' % training_output,\n",
    "        ],\n",
    "        file_outputs = {'train': '/output.txt'}\n",
    "    )\n",
    "\n",
    "def dataflow_tf_model_analyze_op(model: 'TensorFlow model', evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', project: 'GcpProject', analyze_mode, analyze_slice_column, analysis_output: 'GcsUri', step_name='analysis'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tfma:2c2445df83fa879387a200747cc20f72a7ee9727',\n",
    "        arguments = [\n",
    "            '--model', model,\n",
    "            '--eval', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--project', project,\n",
    "            '--mode', analyze_mode,\n",
    "            '--slice-columns', analyze_slice_column,\n",
    "            '--output', '%s/{{workflow.name}}/analysis' % analysis_output,\n",
    "        ],\n",
    "        file_outputs = {'analysis': '/output.txt'}\n",
    "    )\n",
    "\n",
    "\n",
    "def dataflow_tf_predict_op(evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', target: str, model: 'TensorFlow model', predict_mode, project: 'GcpProject', prediction_output: 'GcsUri', step_name='prediction'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tf-predict:a277f87ea1d4707bf860d080d06639b7caf9a1cf',\n",
    "        arguments = [\n",
    "            '--data', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--target', target,\n",
    "            '--model',  model,\n",
    "            '--mode', predict_mode,\n",
    "            '--project', project,\n",
    "            '--output', '%s/{{workflow.name}}/predict' % prediction_output,\n",
    "        ],\n",
    "        file_outputs = {'prediction': '/output.txt'}\n",
    "    )\n",
    "\n",
    "\n",
    "def confusion_matrix_op(predictions: 'GcsUri', output: 'GcsUri', step_name='confusion_matrix'):\n",
    "  return dsl.ContainerOp(\n",
    "      name=step_name,\n",
    "      image='gcr.io/ml-pipeline/ml-pipeline-local-confusion-matrix:a277f87ea1d4707bf860d080d06639b7caf9a1cf',\n",
    "      arguments=[\n",
    "        '--output', '%s/{{workflow.name}}/confusionmatrix' % output,\n",
    "        '--predictions', predictions,\n",
    "        '--target_lambda', \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\",\n",
    "     ])\n",
    "\n",
    "\n",
    "def roc_op(predictions: 'GcsUri', output: 'GcsUri', step_name='roc'):\n",
    "  return dsl.ContainerOp(\n",
    "      name=step_name,\n",
    "      image='gcr.io/ml-pipeline/ml-pipeline-local-roc:a277f87ea1d4707bf860d080d06639b7caf9a1cf',\n",
    "      arguments=[\n",
    "        '--output', '%s/{{workflow.name}}/roc' % output,\n",
    "        '--predictions', predictions,\n",
    "        '--target_lambda', \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\",\n",
    "     ])\n",
    "\n",
    "\n",
    "def kubeflow_deploy_op(model: 'TensorFlow model', tf_server_name, step_name='deploy'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = 'gcr.io/ml-pipeline/ml-pipeline-kubeflow-deployer:a277f87ea1d4707bf860d080d06639b7caf9a1cf',\n",
    "        arguments = [\n",
    "            '--model-export-path', '%s/export/export' % model,\n",
    "            '--server-name', tf_server_name,\n",
    "            '--pvc-name', 'efs-claim',\n",
    "            '--cluster-name', 'eks-cluster'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def taxi_cab_classification(\n",
    "    output,\n",
    "    project,\n",
    "    column_names='/mnt/taxi/column-names.json',\n",
    "    key_columns='trip_start_timestamp',\n",
    "    train='/mnt/taxi/train.csv',\n",
    "    evaluation='/mnt/taxi/eval.csv',\n",
    "    mode='local',\n",
    "    preprocess_module='/mnt/taxi/preprocessing.py',\n",
    "    learning_rate=0.1,\n",
    "    hidden_layer_size='1500',\n",
    "    steps=3000,\n",
    "    analyze_slice_column='trip_start_hour'):\n",
    "\n",
    "  tf_server_name = 'taxi-cab-classification-model-{{workflow.uid}}'\n",
    "  validation = dataflow_tf_data_validation_op(train, evaluation, column_names,\n",
    "      key_columns, project, mode, output\n",
    "  )\n",
    "  preprocess = dataflow_tf_transform_op(train, evaluation, validation.outputs['schema'],\n",
    "      project, mode, preprocess_module, output\n",
    "  )\n",
    "  training = tf_train_op(preprocess.output, validation.outputs['schema'], learning_rate,\n",
    "      hidden_layer_size, steps, 'tips', preprocess_module, output\n",
    "  )\n",
    "  analysis = dataflow_tf_model_analyze_op(training.output, evaluation,\n",
    "      validation.outputs['schema'], project, mode, analyze_slice_column, output\n",
    "  )\n",
    "  prediction = dataflow_tf_predict_op(evaluation, validation.outputs['schema'], 'tips',\n",
    "      training.output, mode, project, output\n",
    "  )\n",
    "  cm = confusion_matrix_op(prediction.output, output)\n",
    "  roc = roc_op(prediction.output, output)\n",
    "  deploy = kubeflow_deploy_op(training.output, tf_server_name)\n",
    "\n",
    "  validation.add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt',name='data-storage')).add_volume(k8s_client.V1Volume(name='data-storage',persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='efs-claim')))\n",
    "  preprocess.add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt',name='data-storage')).add_volume(k8s_client.V1Volume(name='data-storage',persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='efs-claim')))\n",
    "  training.add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt',name='data-storage')).add_volume(k8s_client.V1Volume(name='data-storage',persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='efs-claim')))\n",
    "  analysis.add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt',name='data-storage')).add_volume(k8s_client.V1Volume(name='data-storage',persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='efs-claim')))\n",
    "  prediction.add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt',name='data-storage')).add_volume(k8s_client.V1Volume(name='data-storage',persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='efs-claim')))\n",
    "  cm.add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt',name='data-storage')).add_volume(k8s_client.V1Volume(name='data-storage',persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='efs-claim')))\n",
    "  roc.add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt',name='data-storage')).add_volume(k8s_client.V1Volume(name='data-storage',persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='efs-claim')))\n",
    "  deploy.add_volume_mount(k8s_client.V1VolumeMount(mount_path='/mnt',name='data-storage')).add_volume(k8s_client.V1Volume(name='data-storage',persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='efs-claim')))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  import kfp.compiler as compiler\n",
    "  compiler.Compiler().compile(taxi_cab_classification, __file__ + '.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successful installation, the command dsl-compile should be available. You can use this command to verify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/dsl-compile\r\n"
     ]
    }
   ],
   "source": [
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
